{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras import layers\n",
    "import io \n",
    "import imageio\n",
    "from IPython.display import Image, display\n",
    "from ipywidgets import widgets, Layout, HBox\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and validation datasets\n",
    "train_dataset = np.load(\"train_dataset_128.npy\")\n",
    "val_dataset = np.load(\"val_dataset_128.npy\")\n",
    "\n",
    "# Normalize the data\n",
    "# train_dataset = train_dataset / 255.0\n",
    "# val_dataset = val_dataset / 255.0\n",
    "\n",
    "# Print dataset shapes\n",
    "print(f\"Training Dataset Shapes: {train_dataset.shape}\")\n",
    "print(f\"Validation Dataset Shapes: {val_dataset.shape}\")\n",
    "\n",
    "# Define a helper function to shift the frames\n",
    "def create_shifted_frames(data):\n",
    "    x = data[:, :-1, :, :]\n",
    "    y = data[:, 1:, :, :]\n",
    "    return x, y\n",
    "\n",
    "# Apply the processing function to the datasets\n",
    "x_train, y_train = create_shifted_frames(train_dataset)\n",
    "x_val, y_val = create_shifted_frames(val_dataset)\n",
    "\n",
    "# Print training and validation dataset information\n",
    "print(f\"Training Dataset Shapes: {x_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation Dataset Shapes: {x_val.shape}, {y_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a sample sequence from the dataset\n",
    "%matplotlib inline\n",
    "\n",
    "# Set up the plot\n",
    "fig, axes = plt.subplots(4, 5, figsize=(10, 8))\n",
    "\n",
    "# Select a random data example\n",
    "data_choice = np.random.randint(len(train_dataset))\n",
    "\n",
    "# Plot each frame of the selected example\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    ax.imshow(np.squeeze(train_dataset[data_choice][idx]), cmap=\"gray\")\n",
    "    ax.set_title(f\"Frame {idx + 1}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "print(f\"Displaying frames for example {data_choice}.\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define model information\n",
    "model_info = '_128_SSIM'\n",
    "\n",
    "# Set up the ModelCheckpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=f'conv_lstm_model_128_epoch_{{epoch:02d}}{model_info}.h5', \n",
    "    save_freq='epoch',   \n",
    "    save_weights_only=False,  \n",
    "    verbose=1  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.image import ssim\n",
    "\n",
    "def ssim_loss(true, pred):\n",
    "    \"\"\"\n",
    "    SSIM loss function.\n",
    "    \n",
    "    Args:\n",
    "    true (tensor): Ground truth images.\n",
    "    pred (tensor): Predicted images.\n",
    "    \n",
    "    Returns:\n",
    "    float: SSIM loss value.\n",
    "    \"\"\"\n",
    "    # SSIM is typically calculated on a per image basis, and values range between -1 and 1.\n",
    "    # The original SSIM index is a measure of similarity, but for a loss function, we need a dissimilarity measure.\n",
    "    # We can convert it by subtracting the SSIM from 1.\n",
    "    return 1 - tf.reduce_mean(ssim(true, pred, max_val=1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    \"\"\"\n",
    "    Compute the Dice coefficient, a measure of overlap between the true and predicted binary masks.\n",
    "    \n",
    "    Args:\n",
    "    y_true (tensor): Ground truth binary mask.\n",
    "    y_pred (tensor): Predicted binary mask.\n",
    "    smooth (float): Smoothing factor to avoid division by zero.\n",
    "    \n",
    "    Returns:\n",
    "    float: Dice coefficient.\n",
    "    \"\"\"\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    dice = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return dice\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the Dice loss, which is 1 minus the Dice coefficient.\n",
    "    \n",
    "    Args:\n",
    "    y_true (tensor): Ground truth binary mask.\n",
    "    y_pred (tensor): Predicted binary mask.\n",
    "    \n",
    "    Returns:\n",
    "    float: Dice loss.\n",
    "    \"\"\"\n",
    "    return 1 - dice_coef(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define model information\n",
    "model_info = '_128_SSIM'\n",
    "\n",
    "# Set up the ModelCheckpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=f'conv_lstm_model_128_epoch_{{epoch:02d}}{model_info}.h5', \n",
    "    save_freq='epoch',   \n",
    "    save_weights_only=False,  \n",
    "    verbose=1  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, optimizers\n",
    "from keras import metrics\n",
    "\n",
    "def build_conv_lstm_model(input_shape):\n",
    "    \"\"\"\n",
    "    Build a ConvLSTM model.\n",
    "    \n",
    "    Args:\n",
    "    input_shape (tuple): Shape of the input data (frames_per_sequence, width, height, channels).\n",
    "    \n",
    "    Returns:\n",
    "    model: A compiled Keras model.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(None, *input_shape))\n",
    "\n",
    "    # First level of ConvLSTM\n",
    "    x = layers.ConvLSTM2D(filters=128, kernel_size=(5, 5), padding=\"same\", return_sequences=True, activation=\"relu\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Second level of ConvLSTM\n",
    "    x = layers.ConvLSTM2D(filters=64, kernel_size=(3, 3), padding=\"same\", return_sequences=True, activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Third level of ConvLSTM\n",
    "    x = layers.ConvLSTM2D(filters=64, kernel_size=(1, 1), padding=\"same\", return_sequences=True, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # Conv3D to get the final output frame\n",
    "    outputs = layers.Conv3D(filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\")(x)\n",
    "    \n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Build and compile the model\n",
    "input_shape = x_train.shape[2:]  \n",
    "conv_lstm_model = build_conv_lstm_model(input_shape)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "loss = keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# Compile the model\n",
    "conv_lstm_model.compile(loss=dice_coef_loss, optimizer=optimizer)\n",
    "\n",
    "# Print model summary\n",
    "conv_lstm_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of epochs and batch size\n",
    "epochs = 10\n",
    "batch_size = 2\n",
    "\n",
    "# Train the model using GPU\n",
    "with tf.device('/GPU:0'):\n",
    "    history = conv_lstm_model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[checkpoint_callback],\n",
    "        validation_data=(x_val, y_val)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "loaded_model = tf.keras.models.load_model(\"conv_lstm_model_128_epoch_10_128_dice.h5\", compile=False)\n",
    "\n",
    "\n",
    "# Compile the loaded model\n",
    "loaded_model.compile(loss=dice_coef_loss, optimizer=optimizer)\n",
    "\n",
    "# loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load and normalize the synthetic train and validation datasets\n",
    "syn_train_dataset = np.load(\"synthetic_train_data_128.npy\") / 255.0\n",
    "syn_val_dataset = np.load(\"synthetic_val_data_128.npy\") / 255.0\n",
    "\n",
    "# Ensure the datasets are of type float32\n",
    "syn_train_dataset = syn_train_dataset.astype(np.float32)\n",
    "syn_val_dataset = syn_val_dataset.astype(np.float32)\n",
    "\n",
    "# Inspect the dataset shapes\n",
    "print(f\"Training Dataset Shape: {syn_train_dataset.shape}\")\n",
    "print(f\"Validation Dataset Shape: {syn_val_dataset.shape}\")\n",
    "\n",
    "# Helper function to shift the frames\n",
    "def create_shifted_frames(data):\n",
    "    x = data[:, :-1, :, :, :]\n",
    "    y = data[:, 1:, :, :, :]\n",
    "    return x, y\n",
    "\n",
    "# Apply the processing function to the datasets\n",
    "x_train, y_train = create_shifted_frames(syn_train_dataset)\n",
    "x_val, y_val = create_shifted_frames(syn_val_dataset)\n",
    "\n",
    "# Inspect the shapes of the processed datasets\n",
    "print(f\"Training Dataset Shapes: x_train: {x_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"Validation Dataset Shapes: x_val: {x_val.shape}, y_val: {y_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../gamma_index/ConvLSTM_Model_128_dice.h5\"\n",
    "# model_path = '../gamma_index/finetuned_ConvLSTM_Model_128_dice_10.h5'\n",
    "# loaded_model = load_model(model_path)\n",
    "optimizer = keras.optimizers.Adam()\n",
    "loaded_model = load_model(model_path, compile=False)\n",
    "loaded_model.compile(loss=dice_coef_loss, optimizer=optimizer)\n",
    "loaded_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "#     filepath='../gamma_index/retuned_finetuned_ConvLSTM_Model_128_dice_{epoch:02d}.h5',  \n",
    "    filepath='../gamma_index/finetuned_ConvLSTM_Model_128_dice_{epoch:02d}.h5',  \n",
    "    save_freq='epoch',                              \n",
    "    save_weights_only=False,                          \n",
    "    verbose=1                                         \n",
    ")\n",
    "\n",
    "# Train the model \n",
    "epochs = 10\n",
    "batch_size = 2\n",
    "\n",
    "loaded_model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, \n",
    "              validation_data = (x_val, y_val), callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loop through the specified range of examples in the validation dataset\n",
    "for idx in range(106):\n",
    "    index = idx  # Use the current index directly\n",
    "    example = val_dataset[index]\n",
    "\n",
    "    # Pick the first 19 frames from the example for prediction\n",
    "    frames = example[:19, ...]\n",
    "\n",
    "    # Ground truth for the 20th frame\n",
    "    ground_truth_frame = example[19, ...]\n",
    "\n",
    "    # Predict the 20th frame using the model\n",
    "    new_prediction = loaded_model.predict(np.expand_dims(frames, axis=0))\n",
    "    predicted_20th_frame = np.squeeze(new_prediction[:, -1, ...])\n",
    "\n",
    "    # Plot the ground truth and predicted 20th frames\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Ground truth frame\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(np.squeeze(ground_truth_frame), cmap=\"gray\")\n",
    "    plt.title(f\"Ground Truth (Frame 20) of index: {index}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Predicted frame\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(np.squeeze(predicted_20th_frame), cmap=\"gray\")\n",
    "    plt.title(f\"Predicted (Frame 20) of index: {index}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
